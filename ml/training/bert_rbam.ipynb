{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "!pip install transformers\n",
    "!pip install emoji\n",
    "!pip install datasets\n",
    "\n",
    "# %env WANDB_PROJECT=twitter-roberta-base-dec2021_rbam_fine_tuned\n",
    "# %env WANDB_WATCH=all\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "from datasets import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
    "\n",
    "from google.colab import drive, files\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "\n",
    "class_weights = torch.tensor([0.6191, 0.7282, 0.6526], device='cuda:0')\n",
    "\n",
    "class WeightedLossTrainer(Trainer):\n",
    "\n",
    "  def compute_loss(self, model, inputs, return_outputs=False):\n",
    "    # Feed input into model and extract logits\n",
    "    outputs = model(**inputs)\n",
    "    logits = outputs.get(\"logits\")\n",
    "    # Extract Labels\n",
    "    labels = inputs.get(\"labels\")\n",
    "    # Define loss function with class weights\n",
    "    loss_func = nn.CrossEntropyLoss(weight=class_weights)\n",
    "    # Compute loss\n",
    "    loss = loss_func(logits, labels)\n",
    "    return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "\n",
    "class BertRBAM:\n",
    "\n",
    "  DATASET_PATH = 'drive/MyDrive/Colab Notebooks/prj/dataset.csv'\n",
    "\n",
    "  MODEL = \"cardiffnlp/twitter-roberta-base-dec2021\"\n",
    "  tokenizer = AutoTokenizer.from_pretrained(MODEL, padding=\"max_length\", truncation=True, max_length=512)\n",
    "\n",
    "  id2label = {0: \"attack\", 1: \"neutral\", 2: \"support\"}\n",
    "  label2id = {\"attack\": 0, \"neutral\": 1, \"support\": 2}\n",
    "\n",
    "  hyperparameters = {'num_train_epochs': 2, 'learning_rate': 2e-5, 'weight_decay': 0.01, 'batch_size': 8}\n",
    "  OUTPUT_DIR =\"twitter-roberta-base-dec2021_rbam_fine_tuned\"\n",
    "\n",
    "\n",
    "  def __init__(self):\n",
    "    self.df = pd.read_csv(BertRBAM.DATASET_PATH)\n",
    "    self.preprocess_dataset()\n",
    "\n",
    "    self.X_train, self.y_train, self.X_valid, self.y_valid, self.X_test, self.y_test = self.get_split_dataset()\n",
    "\n",
    "    self.df_train, self.df_valid, self.df_test, self.tokenized_dataset_train, self.tokenized_dataset_test, self.tokenized_dataset_valid = self.tokenize_dataset()\n",
    "\n",
    "\n",
    "    self.class_weights = (1 - (self.df_train[\"labels\"].value_counts().sort_index() / len(self.df_train))).values\n",
    "    self.class_weights = torch.from_numpy(self.class_weights).float().to(\"cuda\")\n",
    "    global class_weights\n",
    "    class_weights = self.class_weights\n",
    "\n",
    "    self.steps_per_epoch = len(self.df_train) // BertRBAM.hyperparameters['batch_size']\n",
    "\n",
    "    self.trainer = None\n",
    "    self.evaluation_metrics = None\n",
    "\n",
    "  def preprocess(self, text):\n",
    "    preprocessed_text = []\n",
    "    original_text_words = text.split(\" \")\n",
    "    for word in original_text_words:\n",
    "        word = 'http' if text.startswith('http') else word  # Preprocess links\n",
    "        word = '@user' if text.startswith('@') and len(word) > 1 else word  # Preprocess user handles\n",
    "        preprocessed_text.append(word)\n",
    "    return \" \".join(preprocessed_text)\n",
    "\n",
    "  def get_split_dataset(self):\n",
    "    train_size = 0.8\n",
    "    test_size = 0.5\n",
    "    X = self.df[['text_a', 'text_b']]\n",
    "    y = self.df[['labels']]\n",
    "\n",
    "    X_train, X_rem, y_train, y_rem = train_test_split(X, y, train_size=train_size, random_state=42)\n",
    "\n",
    "    X_valid, X_test, y_valid, y_test = train_test_split(X_rem,y_rem, test_size=test_size, random_state=42)\n",
    "\n",
    "    return X_train, y_train, X_valid, y_valid, X_test, y_test\n",
    "\n",
    "  def preprocess_dataset(self):\n",
    "    self.df['text_a'] = self.df['text_a'].apply(lambda x : self.preprocess(x))\n",
    "    self.df['text_b'] = self.df['text_b'].apply(lambda x : self.preprocess(x))\n",
    "\n",
    "\n",
    "  def tokenize(self, example):\n",
    "    text_a = example['text_a']\n",
    "    text_b = example['text_b']\n",
    "\n",
    "    return BertRBAM.tokenizer(text_a, text_b, padding=\"max_length\", truncation=True, max_length=512)\n",
    "\n",
    "\n",
    "  def tokenize_dataset(self):\n",
    "    df_train = pd.concat([self.X_train, self.y_train], axis=1)\n",
    "    df_test = pd.concat([self.X_test, self.y_test], axis=1)\n",
    "    df_valid = pd.concat([self.X_valid, self.y_valid], axis=1)\n",
    "\n",
    "    dataset_train = Dataset.from_pandas(df_train)\n",
    "    dataset_test = Dataset.from_pandas(df_test)\n",
    "    dataset_valid = Dataset.from_pandas(df_valid)\n",
    "\n",
    "    tokenized_dataset_train = dataset_train.map(self.tokenize, batched=True)\n",
    "    tokenized_dataset_test = dataset_test.map(self.tokenize, batched=True)\n",
    "    tokenized_dataset_valid = dataset_valid.map(self.tokenize, batched=True)\n",
    "\n",
    "    tokenized_dataset_train = tokenized_dataset_train.remove_columns(['text_a', 'text_b', '__index_level_0__', ])\n",
    "    tokenized_dataset_test = tokenized_dataset_test.remove_columns(['text_a', 'text_b', '__index_level_0__', ])\n",
    "    tokenized_dataset_valid = tokenized_dataset_valid.remove_columns(['text_a', 'text_b', '__index_level_0__', ])\n",
    "\n",
    "    tokenized_dataset_train = tokenized_dataset_train.with_format('torch')\n",
    "    tokenized_dataset_test = tokenized_dataset_test.with_format('torch')\n",
    "    tokenized_dataset_valid = tokenized_dataset_valid.with_format('torch')\n",
    "\n",
    "    return df_train, df_valid, df_test, tokenized_dataset_train, tokenized_dataset_test, tokenized_dataset_valid\n",
    "\n",
    "\n",
    "  def compute_metrics(self, pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "\n",
    "    accuracy = accuracy_score(y_true=labels, y_pred=preds)\n",
    "    recall = recall_score(y_true=labels, y_pred=preds, average=\"weighted\")\n",
    "    precision = precision_score(y_true=labels, y_pred=preds, average=\"weighted\")\n",
    "    f1 = f1_score(labels, preds, average=\"weighted\")\n",
    "\n",
    "    # class_names = [\"attack\", \"neutral\", \"support\"]\n",
    "\n",
    "    # wandb.log({\"conf_mat\" : wandb.plot.confusion_matrix(probs=None,\n",
    "    #                           preds=preds, y_true=labels,\n",
    "    #                           class_names=class_names)})\n",
    "\n",
    "    return {\"accuracy\": accuracy, \"precision\": precision, \"recall\": recall, \"f1\": f1}\n",
    "\n",
    "  def model_init(self):\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(BertRBAM.MODEL, num_labels=3, id2label=BertRBAM.id2label, label2id=BertRBAM.label2id)\n",
    "    return model\n",
    "\n",
    "  def train_model(self):\n",
    "    training_args = TrainingArguments(\n",
    "                                  output_dir=BertRBAM.OUTPUT_DIR,\n",
    "                                  num_train_epochs=BertRBAM.hyperparameters['num_train_epochs'],\n",
    "                                  learning_rate=BertRBAM.hyperparameters['learning_rate'],\n",
    "                                  weight_decay=BertRBAM.hyperparameters['weight_decay'],\n",
    "                                  per_device_train_batch_size=BertRBAM.hyperparameters['batch_size'],\n",
    "                                  per_device_eval_batch_size=BertRBAM.hyperparameters['batch_size'],\n",
    "                                  save_strategy=\"epoch\",\n",
    "                                  evaluation_strategy=\"epoch\",\n",
    "                                  load_best_model_at_end=True,\n",
    "                                  metric_for_best_model=\"eval_f1\",\n",
    "                                  remove_unused_columns = False,\n",
    "                                  logging_steps=self.steps_per_epoch,\n",
    "                                  log_level=\"warning\",\n",
    "                                  # report_to=\"wandb\",\n",
    "                                  # push_to_hub=True\n",
    "                                  )\n",
    "\n",
    "    self.trainer = WeightedLossTrainer(model_init=self.model_init,\n",
    "                              args=training_args,\n",
    "                              compute_metrics=self.compute_metrics,\n",
    "                              train_dataset=self.tokenized_dataset_train,\n",
    "                              eval_dataset=self.tokenized_dataset_valid)\n",
    "\n",
    "    self.trainer.train()\n",
    "\n",
    "    self.evaluation_metrics = self.trainer.evaluate(eval_dataset=self.tokenized_dataset_test)\n",
    "\n",
    "    print(self.evaluation_metrics)\n",
    "\n",
    "\n",
    "bert_rbam_model = BertRBAM()\n",
    "eval_metrics = bert_rbam_model.train_model()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}